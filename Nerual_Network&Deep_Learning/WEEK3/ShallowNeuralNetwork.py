## 12/18/2019 Wed ##
## Struggled a lot with the project of Week 2 ##
## Some kind of pessimistic about the upcoming contents ##

# Neural Network Overview #

# Neural Network Representation #
# Two-layer Neural Network 
# Input layer
# Hidden layer
# Output  

# Computing a Neural Network's Output #
# Node and layer

# Vectorizing across Multiple Examples #


# Activation Functions #
# Sigmoid function 
# tanh 
# ReLU: Rectified Linear Unit
# Leaky ReLU


## 12/19/2019 ##
# Activtion Function ##
# House Price prediction with ReLU

#  Derivation of activation function #
# Slope

# Gradient descent for Neural Networks #
# Formula for computing derivatives
# Forward propagation 
# Z[1] = w[1]*x + b[1]
# A[1] = g[1]*z[1]
# Z[2] = w[2]*A[1]+ b[1]
# A[2] = g[2]*z[2]= sigmoid(z[2])
